{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\wel\n",
      "[nltk_data]     come\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\wel\n",
      "[nltk_data]     come\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:\n",
      "Labels array  <class 'pandas.core.series.Series'>\n",
      "Comments array  <class 'pandas.core.series.Series'>\n",
      "one comment line  <class 'str'>\n",
      "Shape:\n",
      "Labels array  (1010773,)\n",
      "Comments array  (1010773,)\n",
      "Two first entries:\n",
      "0 NC and NH.\n",
      "0 You do know west teams play against west teams more than east teams right?\n"
     ]
    }
   ],
   "source": [
    "# df - data frame\n",
    "df = pd.read_csv('C:/Akshata/Courses/Machine_Learning/Final_Project/sarcasm/train-balanced-sarcasm.csv')\n",
    "# dropping empty comment entries\n",
    "df.dropna(subset=['comment'], inplace=True)\n",
    "\n",
    "print('Type:')\n",
    "print('Labels array ',type(df.label))\n",
    "print('Comments array ',type(df.comment))\n",
    "print('one comment line ', type(df.comment[0]))\n",
    "print('Shape:')\n",
    "print('Labels array ',df.label.shape)\n",
    "print('Comments array ',df.comment.shape)\n",
    "print('Two first entries:')\n",
    "print (df.label[0], df.comment[0])\n",
    "print (df.label[1], df.comment[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167435\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# create the transform\n",
    "tf_idf_vectorizer = TfidfVectorizer()\n",
    "# tokenize and build vocab\n",
    "T = tf_idf_vectorizer.fit(df.comment)\n",
    "print(len(T.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess with nltk\n",
    "def my_tokenizer(corpus):\n",
    "    corpus_tokenized = []\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    sbs = nltk.stem.SnowballStemmer('english', ignore_stopwords=False)\n",
    "    for comment in corpus:\n",
    "        words = tokenizer.tokenize(comment)\n",
    "        # Remove stop words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words = [word for word in words if not word in stop_words]\n",
    "        \n",
    "        cmnt_t = []\n",
    "        for token in words:\n",
    "            cmnt_t.append(sbs.stem(token))\n",
    "            # make a string to be compatible with TfidfVectorizer\n",
    "            c = ' '.join(cmnt_t)\n",
    "        # Lemmitize\n",
    "        #words = [lemmatizer.lemmatize(word) for word in words]\n",
    "        \n",
    "        corpus_tokenized.append(c)\n",
    "    return corpus_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = my_tokenizer(df.comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NC and NH.\n",
      "nc nh \n",
      "\n",
      "You do know west teams play against west teams more than east teams right?\n",
      "you know west team play west team east team right \n",
      "\n",
      "They were underdogs earlier today, but since Gronk's announcement this afternoon, the Vegas line has moved to patriots -1\n",
      "they underdog earlier today sinc gronk announc afternoon vega line move patriot 1 \n",
      "\n",
      "This meme isn't funny none of the \"new york nigga\" ones are.\n",
      "this meme funni none new york nigga one \n",
      "\n",
      "I could use one of those tools.\n",
      "i could use one tool \n",
      "\n",
      "I don't pay attention to her, but as long as she's legal I wouldn't kick her out of bed (before she took a load)\n",
      "i pay attent long legal i kick bed took load \n",
      "\n",
      "Trick or treating in general is just weird...\n",
      "trick treat general weird \n",
      "\n",
      "Blade Mastery+Masamune or GTFO!\n",
      "blade masteri masamun gtfo \n",
      "\n",
      "You don't have to, you have a good build, buy games or save it\n",
      "you good build buy game save \n",
      "\n",
      "I would love to see him at lolla.\n",
      "i would love see lolla \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(df.comment[i])\n",
    "    print(df2[i], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nc nh', 'you know west team play west team east team right', 'they underdog earlier today sinc gronk announc afternoon vega line move patriot 1', 'this meme funni none new york nigga one', 'i could use one tool'] \n",
      "\n",
      "<class 'pandas.core.series.Series'> \n",
      "\n",
      "<class 'str'>\n",
      "0                                                nc nh\n",
      "1    you know west team play west team east team right\n",
      "2    they underdog earlier today sinc gronk announc...\n",
      "3              this meme funni none new york nigga one\n",
      "4                                 i could use one tool\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df2[0:5], '\\n')\n",
    "# convert to pandas Series type type to be compatible with TfidfVectorizer\n",
    "df3 = pd.Series((v for v in df2))\n",
    "print(type(df3), '\\n')\n",
    "print(type(df3[0]))\n",
    "print(df3[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131021\n"
     ]
    }
   ],
   "source": [
    "tf_idf_vectorizer2 = TfidfVectorizer()\n",
    "T2 = tf_idf_vectorizer2.fit(df3)\n",
    "print(len(T2.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wel come\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# divide into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.comment, df.label, train_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(333555, 131021)\n",
      "  (0, 108352)\t1.0\n",
      "  (1, 129692)\t0.38859406362887755\n",
      "  (1, 116331)\t0.38496751073909\n",
      "  (1, 84472)\t0.25856422318991007\n",
      "  (1, 81766)\t0.2996483874033827\n",
      "  (1, 69660)\t0.420801991589258\n",
      "  (1, 54571)\t0.4588915070041935\n",
      "  (1, 53988)\t0.3955936355332698\n",
      "  (2, 126959)\t0.3911075447672028\n",
      "  (2, 109525)\t0.313744880672759\n",
      "  (2, 85612)\t0.4465877960200291\n",
      "  (2, 62866)\t0.31698165141686463\n",
      "  (2, 53870)\t0.2843569055062332\n",
      "  (2, 23999)\t0.5036472618265457\n",
      "  (2, 22865)\t0.33787808057694224\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Vetorize the training data set\n",
    "X_train = tf_idf_vectorizer2.transform(X_train)\n",
    "print(X_train.shape)\n",
    "print(X_train[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(677218, 167435)\n",
      "  (0, 162863)\t0.2158504879367724\n",
      "  (0, 161502)\t0.2296506540092197\n",
      "  (0, 149355)\t0.14639316874527655\n",
      "  (0, 147395)\t0.16056372407840933\n",
      "  (0, 112716)\t0.6457335157038966\n",
      "  (0, 95593)\t0.48204741331222684\n",
      "  (0, 68465)\t0.20913216025879783\n",
      "  (0, 45077)\t0.31340761978078674\n",
      "  (0, 44923)\t0.24930983945663132\n",
      "  (1, 157333)\t0.346245312694975\n",
      "  (1, 149355)\t0.09976887477073469\n",
      "  (1, 147462)\t0.08425584863802556\n",
      "  (1, 147395)\t0.21885259015296077\n",
      "  (1, 107569)\t0.3370808805385962\n",
      "  (1, 87590)\t0.14924998759330788\n",
      "  (1, 74372)\t0.2763643816865998\n",
      "  (1, 70649)\t0.2621129774888751\n",
      "  (1, 54465)\t0.41018268646107664\n",
      "  (1, 49106)\t0.2542007546482659\n",
      "  (1, 46664)\t0.31283044276820554\n",
      "  (1, 36315)\t0.19731124432121439\n",
      "  (1, 18945)\t0.13936183407788402\n",
      "  (1, 10585)\t0.38092137478897886\n",
      "  (2, 145082)\t0.2775923084122987\n",
      "  (2, 126293)\t0.4835977538920855\n",
      "  (2, 114751)\t0.5446730745893612\n",
      "  (2, 106800)\t0.18136352222323712\n",
      "  (2, 103526)\t0.20425332459220988\n",
      "  (2, 100383)\t0.2076392686896697\n",
      "  (2, 77895)\t0.1388006814645435\n",
      "  (2, 59163)\t0.5053861114841223\n"
     ]
    }
   ],
   "source": [
    "# Vetorize the test data set\n",
    "X_test = tf_idf_vectorizer.transform(X_test)\n",
    "print(X_test.shape)\n",
    "print(X_test[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oob_score is  0.7137173779436674\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "#from sklearn import svm\n",
    "#from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# bagging with SVM\n",
    "# oob Out-Of-Bag\n",
    "bag_log_reg = BaggingClassifier(\n",
    "     LinearSVC(), n_estimators=50,\n",
    "    max_samples=50000, bootstrap=True, n_jobs=-1, oob_score=True)\n",
    "\n",
    "bag_log_reg.fit(X_train,y_train)\n",
    "print('oob_score is ', bag_log_reg.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oob_score is  0.7151144488914871\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rnd_frst = RandomForestClassifier(\n",
    "      n_estimators=100, max_leaf_nodes=1000, n_jobs=-1, oob_score=True)\n",
    "rnd_frst.fit(X_train,y_train)\n",
    "print('oob_score is ', bag_log_reg.oob_score_)\n",
    "\n",
    "# 5-folds cross validation with grid search\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# I tried with n_jobs=-1 but kernel died after a few hours\n",
    "params = [{'n_estimators': [100, 200], 'min_samples_leaf': [1, 10, 50, 100],\n",
    "                     'random_state': [47], 'n_jobs': [2]}]\n",
    "rnd_frst = GridSearchCV(RandomForestClassifier(), params, cv=5, scoring='accuracy')\n",
    "\n",
    "#clf = RandomForestClassifier(n_estimators=200, min_samples_leaf=50, random_state=31, n_jobs = -1)\n",
    "rnd_frst.fit(X_train, y_train)\n",
    "print('oob_score is ', rnd_frst.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
