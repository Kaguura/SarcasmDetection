{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Luba\n",
      "[nltk_data]     Tovbin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Luba\n",
      "[nltk_data]     Tovbin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@unpublished{SARC,\n",
    "#  authors={Mikhail Khodak and Nikunj Saunshi and Kiran Vodrahalli},\n",
    "#  title={A Large Self-Annotated Corpus for Sarcasm},\n",
    "#  url={https://arxiv.org/abs/1704.05579},\n",
    "#  year=2017\n",
    "#}\n",
    "#https://www.kaggle.com/danofer/sarcasm#train-balanced-sarcasm.csv\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:\n",
      "Labels array  <class 'pandas.core.series.Series'>\n",
      "Comments array  <class 'pandas.core.series.Series'>\n",
      "one comment line  <class 'str'>\n",
      "Shape:\n",
      "Labels array  (1010773,)\n",
      "Comments array  (1010773,)\n",
      "Two first entries:\n",
      "0 NC and NH.\n",
      "0 You do know west teams play against west teams more than east teams right?\n"
     ]
    }
   ],
   "source": [
    "# df - data frame\n",
    "df = pd.read_csv('C:/Users/Luba Tovbin/Desktop/CMPE-257/Team_Proj_Sarcasm/sarcasm/train-balanced-sarcasm.csv')\n",
    "# dropping empty comment entries\n",
    "df.dropna(subset=['comment'], inplace=True)\n",
    "\n",
    "print('Type:')\n",
    "print('Labels array ',type(df.label))\n",
    "print('Comments array ',type(df.comment))\n",
    "print('one comment line ', type(df.comment[0]))\n",
    "print('Shape:')\n",
    "print('Labels array ',df.label.shape)\n",
    "print('Comments array ',df.comment.shape)\n",
    "print('Two first entries:')\n",
    "print (df.label[0], df.comment[0])\n",
    "print (df.label[1], df.comment[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167435\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# create the transform\n",
    "tf_idf_vectorizer = TfidfVectorizer()\n",
    "# tokenize and build vocab\n",
    "T = tf_idf_vectorizer.fit(df.comment)\n",
    "print(len(T.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess with nltk\n",
    "def my_tokenizer(corpus):\n",
    "    corpus_tokenized = []\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    sbs = nltk.stem.SnowballStemmer('english', ignore_stopwords=False)\n",
    "    for comment in corpus:\n",
    "        words = tokenizer.tokenize(comment)\n",
    "        cmnt_t = []\n",
    "        for token in words:\n",
    "            cmnt_t.append(sbs.stem(token))\n",
    "            # make a string to be compatible with TfidfVectorizer\n",
    "            c = ' '.join(cmnt_t)\n",
    "        corpus_tokenized.append(c)\n",
    "    return corpus_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = my_tokenizer(df.comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NC and NH.\n",
      "nc and nh \n",
      "\n",
      "You do know west teams play against west teams more than east teams right?\n",
      "you do know west team play against west team more than east team right \n",
      "\n",
      "They were underdogs earlier today, but since Gronk's announcement this afternoon, the Vegas line has moved to patriots -1\n",
      "they were underdog earlier today but sinc gronk s announc this afternoon the vega line has move to patriot 1 \n",
      "\n",
      "This meme isn't funny none of the \"new york nigga\" ones are.\n",
      "this meme isn t funni none of the new york nigga one are \n",
      "\n",
      "I could use one of those tools.\n",
      "i could use one of those tool \n",
      "\n",
      "I don't pay attention to her, but as long as she's legal I wouldn't kick her out of bed (before she took a load)\n",
      "i don t pay attent to her but as long as she s legal i wouldn t kick her out of bed befor she took a load \n",
      "\n",
      "Trick or treating in general is just weird...\n",
      "trick or treat in general is just weird \n",
      "\n",
      "Blade Mastery+Masamune or GTFO!\n",
      "blade masteri masamun or gtfo \n",
      "\n",
      "You don't have to, you have a good build, buy games or save it\n",
      "you don t have to you have a good build buy game or save it \n",
      "\n",
      "I would love to see him at lolla.\n",
      "i would love to see him at lolla \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(df.comment[i])\n",
    "    print(df2[i], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nc and nh', 'you do know west team play against west team more than east team right', 'they were underdog earlier today but sinc gronk s announc this afternoon the vega line has move to patriot 1', 'this meme isn t funni none of the new york nigga one are', 'i could use one of those tool'] \n",
      "\n",
      "<class 'pandas.core.series.Series'> \n",
      "\n",
      "<class 'str'>\n",
      "0                                            nc and nh\n",
      "1    you do know west team play against west team m...\n",
      "2    they were underdog earlier today but sinc gron...\n",
      "3    this meme isn t funni none of the new york nig...\n",
      "4                        i could use one of those tool\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df2[0:5], '\\n')\n",
    "# convert to pandas Series type type to be compatible with TfidfVectorizer\n",
    "df3 = pd.Series((v for v in df2))\n",
    "print(type(df3), '\\n')\n",
    "print(type(df3[0]))\n",
    "print(df3[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131022\n"
     ]
    }
   ],
   "source": [
    "tf_idf_vectorizer2 = TfidfVectorizer()\n",
    "T2 = tf_idf_vectorizer2.fit(df3)\n",
    "print(len(T2.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "167435 features from before now reduced to 131022 features, that is about 22%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# divide into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.comment, df.label, train_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(333555, 131022)\n",
      "  (0, 114779)\t0.1793130137737192\n",
      "  (0, 58644)\t0.9837920730984704\n",
      "  (1, 114779)\t0.2512595263266975\n",
      "  (1, 80766)\t0.9679197541274193\n",
      "  (2, 127537)\t0.18495877911554362\n",
      "  (2, 126592)\t0.1436433977478119\n",
      "  (2, 121207)\t0.14403305142069298\n",
      "  (2, 114779)\t0.13743171050627323\n",
      "  (2, 114719)\t0.08868106246619416\n",
      "  (2, 95725)\t0.36071037297517844\n",
      "  (2, 85499)\t0.16344319143927794\n",
      "  (2, 83739)\t0.09397292171976926\n",
      "  (2, 80846)\t0.1613647315882898\n",
      "  (2, 71548)\t0.23700624386247882\n",
      "  (2, 68870)\t0.11984800205110516\n",
      "  (2, 60384)\t0.09157056666960955\n",
      "  (2, 58715)\t0.10027337947697297\n",
      "  (2, 49674)\t0.1432978982615192\n",
      "  (2, 44760)\t0.22451918581003313\n",
      "  (2, 40784)\t0.1790493202911679\n",
      "  (2, 35441)\t0.1669070294508233\n",
      "  (2, 33059)\t0.25360823639242763\n",
      "  (2, 29534)\t0.2522865539551292\n",
      "  (2, 26464)\t0.2214490172017322\n",
      "  (2, 22865)\t0.1235961067028143\n",
      "  (2, 16301)\t0.4157599963225752\n",
      "  (2, 12882)\t0.29800211514232305\n",
      "  (2, 10707)\t0.17940520752579436\n"
     ]
    }
   ],
   "source": [
    "# Vetorize the training data set\n",
    "X_train = tf_idf_vectorizer2.transform(X_train)\n",
    "print(X_train.shape)\n",
    "print(X_train[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(677218, 167435)\n",
      "  (0, 147353)\t0.42468250459405066\n",
      "  (0, 135515)\t0.6237320637071776\n",
      "  (0, 121397)\t0.5117743421335358\n",
      "  (0, 103526)\t0.3065539156995408\n",
      "  (0, 68465)\t0.27334008578760505\n",
      "  (1, 136405)\t0.3003210486437824\n",
      "  (1, 127586)\t0.30946562001365147\n",
      "  (1, 117995)\t0.4490575503323284\n",
      "  (1, 115370)\t0.2656883076897965\n",
      "  (1, 114176)\t0.2745484447420461\n",
      "  (1, 106800)\t0.1471396221493557\n",
      "  (1, 74902)\t0.12746855084064718\n",
      "  (1, 60734)\t0.20408522519434966\n",
      "  (1, 53175)\t0.3201515492324172\n",
      "  (1, 34530)\t0.3698623578316731\n",
      "  (1, 29842)\t0.2618033452953899\n",
      "  (1, 18307)\t0.28112851168170877\n",
      "  (2, 70503)\t0.33850766790092285\n",
      "  (2, 7708)\t0.9409636330763684\n"
     ]
    }
   ],
   "source": [
    "# Vetorize the test data set\n",
    "X_test = tf_idf_vectorizer.transform(X_test)\n",
    "print(X_test.shape)\n",
    "print(X_test[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oob_score is  0.6585060934478572\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "# bagging with Logistic Regressoin\n",
    "# oob Out-Of-Bag\n",
    "bag_log_reg = BaggingClassifier(\n",
    "      LogisticRegression(), n_estimators=50,\n",
    "    max_samples=50000, bootstrap=True, n_jobs=-1, oob_score=True)\n",
    "\n",
    "bag_log_reg.fit(X_train,y_train)\n",
    "print('oob_score is ', bag_log_reg.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-239-cc12bcf963b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m rnd_frst = RandomForestClassifier(\n\u001b[0;32m      3\u001b[0m       n_estimators=100, max_leaf_nodes=1000, n_jobs=-1, oob_score=True)\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mrnd_frst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'oob_score is '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbag_log_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moob_score_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moob_score\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 339\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_oob_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m         \u001b[1;31m# Decapsulate classes_ attributes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36m_set_oob_score\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    446\u001b[0m                 estimator.random_state, n_samples)\n\u001b[0;32m    447\u001b[0m             p_estimator = estimator.predict_proba(X[unsampled_indices, :],\n\u001b[1;32m--> 448\u001b[1;33m                                                   check_input=False)\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    831\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tree_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    832\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 833\u001b[1;33m         \u001b[0mproba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    834\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32msklearn\\tree\\_tree.pyx\u001b[0m in \u001b[0;36msklearn.tree._tree.Tree.predict\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msklearn\\tree\\_tree.pyx\u001b[0m in \u001b[0;36msklearn.tree._tree.Tree.predict\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36mget_shape\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_matrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m         \u001b[1;34m\"\"\"Get shape of a matrix.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rnd_frst = RandomForestClassifier(\n",
    "      n_estimators=100, max_leaf_nodes=1000, n_jobs=-1, oob_score=True)\n",
    "rnd_frst.fit(X_train,y_train)\n",
    "print('oob_score is ', bag_log_reg.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
